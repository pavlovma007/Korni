# Форматы экспорта 

## Line 

размер 

	$ sqlite3 kladr.db -line 'select * from  DOMA;' | wc -lc 
	33575552 615520255

**615МБайт**

Внутри это построчное хранение **ТОЛЬКО OUT**

	$ sqlite3 kladr.db -line 'select * from  DOMA;' | head -n 50
	  NAME = 15Лстр1,4Б,8Астр10,8Астр426
	  KORP = 
	  SOCR = ДОМ
	  CODE = 0100000100000030001
	IINDEX = 385000
	GNINMB = 0100
	   UNO = 
	 OCATD = 79401000000
	
	  NAME = 1,10,103,11В,12,15,15Б,16,17,17А,20,22
	  KORP = 
	  SOCR = ДОМ
	  CODE = 0100000100000030002
	IINDEX = 385006
	GNINMB = 0100
	   UNO = 
	 OCATD = 79401000000



## LIST

	$ sqlite3 kladr.db -list 'select * from  DOMA;' --header | wc -lc 
	3730617 343185215

**343МБайт** не плохой результат, но формат "не очевидный"

	NAME|KORP|SOCR|CODE|IINDEX|GNINMB|UNO|OCATD
	15Лстр1,4Б,8Астр10,8Астр426||ДОМ|0100000100000030001|385000|0100||79401000000
	1,10,103,11В,12,15,15Б,16,17,17А,20,22||ДОМ|0100000100000030002|385006|0100||79401000000
	23,3,39,39А,47А,4Б/1,4Бстр10,4Бстр11||ДОМ|0100000100000030003|385006|0100||79401000000



## INSERT 

	$ sqlite3 kladr.db '.dump DOMA' | wc -lc 
	3730630 499871351


таблица занимает 3_730_630  строк  и 499_871_351 байт в insert формате **~500МБайт**. Сколько она занимает в других форматах ? 

внутри выглядит так 

	INSERT INTO DOMA VALUES('15Лстр1,4Б,8Астр10,8Астр426','','ДОМ','0100000100000030001','385000','0100','','79401000000');
	INSERT INTO DOMA VALUES('1,10,103,11В,12,15,15Б,16,17,17А,20,22','','ДОМ','0100000100000030002','385006','0100','','79401000000');
	INSERT INTO DOMA VALUES('23,3,39,39А,47А,4Б/1,4Бстр10,4Бстр11','','ДОМ','0100000100000030003','385006','0100','','79401000000');
	INSERT INTO DOMA VALUES('4Бстр12,4Бстр13,4Бстр14,4Бстр15,4Бстр16','','ДОМ','0100000100000030004','385006','0100','','79401000000');
	INSERT INTO DOMA VALUES('4Бстр17,4Бстр18,4Бстр19,4Бстр2,4Бстр20','','ДОМ','0100000100000030005','385006','0100','','79401000000');
	INSERT INTO DOMA VALUES('4Бстр21,4Бстр22,4Бстр23,4Бстр24,4Бстр25','','ДОМ','0100000100000030006','385006','0100','','79401000000');
	INSERT INTO DOMA VALUES('4Бстр26,4Бстр27,4Бстр28,4Бстр29,4Бстр3','','ДОМ','0100000100000030007','385006','0100','','79401000000');
	INSERT INTO DOMA VALUES('4Бстр30,4Бстр31,4Бстр32,4Бстр4,4Бстр5','','ДОМ','0100000100000030008','385006','0100','','79401000000');
	INSERT INTO DOMA VALUES('4Бстр6,4Бстр7,4Бстр8,4Бстр9,5,6,6стр85,7','','ДОМ','0100000100000030009','385006','0100','','79401000000');
	INSERT INTO DOMA VALUES('8,8А,8Астр145,8Астр196,9,двлд4','','ДОМ','0100000100000030010','385006','0100','','79401000000');
	INSERT INTO DOMA VALUES('109,111,113,115,117,119,121,123,124,125','','ДОМ','0100000100000050001','385006','0100','','79401000000');
	INSERT INTO DOMA VALUES('126,127,128,129,129А,130,131,132,133','','ДОМ','0100000100000050002','385006','0100','','79401000000');


## CSV 

	$ sqlite3 -header -csv kladr.db "select * from DOMA;" | wc -lc 
	3730618 370852249

370_852_249  **371МБайт** байт это меньше почти 500 МБайт

внутри оно выглядит так 

	NAME,KORP,SOCR,CODE,IINDEX,GNINMB,UNO,OCATD
	"15Лстр1,4Б,8Астр10,8Астр426","","ДОМ",0100000100000030001,385000,0100,"",79401000000
	"1,10,103,11В,12,15,15Б,16,17,17А,20,22","","ДОМ",0100000100000030002,385006,0100,"",79401000000
	"23,3,39,39А,47А,4Б/1,4Бстр10,4Бстр11","","ДОМ",0100000100000030003,385006,0100,"",79401000000
	"4Бстр12,4Бстр13,4Бстр14,4Бстр15,4Бстр16","","ДОМ",0100000100000030004,385006,0100,"",79401000000

## JSON-NL

	$ sqlite3 -header -csv kladr.db "select * from DOMA;" | spyql "select * FROM csv TO json" 2>/dev/null |  wc -lc
	3730617 801500223

**801МБайт** - стало больше, но 
к сведению, кодирование внутри там такое  (на юникод прожорливо)

	{"NAME": "15\u041b\u0441\u0442\u04401,4\u0411,8\u0410\u0441\u0442\u044010,8\u0410\u0441\u0442\u0440426", "KORP": "", "SOCR": "\u0414\u041e\u041c", "CODE": 100000100000030001, "IINDEX": 385000, "GNINMB": 100, "UNO": "", "OCATD": 79401000000}
	{"NAME": "1,10,103,11\u0412,12,15,15\u0411,16,17,17\u0410,20,22", "KORP": "", "SOCR": "\u0414\u041e\u041c", "CODE": 100000100000030002, "IINDEX": 385006, "GNINMB": 100, "UNO": "", "OCATD": 79401000000}
	{"NAME": "23,3,39,39\u0410,47\u0410,4\u0411/1,4\u0411\u0441\u0442\u044010,4\u0411\u0441\u0442\u044011", "KORP": "", "SOCR": "\u0414\u041e\u041c", "CODE": 100000100000030003, "IINDEX": 385006, "GNINMB": 100, "UNO": "", "OCATD": 79401000000}

чтобы получить нормальное юникодное кодирование надо 

	pip3 install orjson
	$ sqlite3 -header -csv kladr.db "select * from DOMA;" | spyql "select * FROM csv TO orjson" 2>/dev/null | wc -lc
	3730617 605264585

получим формат типа 

	{"NAME":"15Лстр1,4Б,8Астр10,8Астр426","KORP":"","SOCR":"ДОМ","CODE":100000100000030001,"IINDEX":385000,"GNINMB":100,"UNO":"","OCATD":79401000000}
	{"NAME":"1,10,103,11В,12,15,15Б,16,17,17А,20,22","KORP":"","SOCR":"ДОМ","CODE":100000100000030002,"IINDEX":385006,"GNINMB":100,"UNO":"","OCATD":79401000000}

тут символов уже меньше, а байтов будет  605МБайт

**НО!**  этот формат не так уж плох. 

> `doma.csv.gz` и `doma.jsonnl.gz`  в размере отличаются не значительно. 49,9МБ и 58,4МБ соответственно. 
>
> Это важно. у jsonnl много преимуществ перед csv этот формат лучше подходит если структура файла может изменятся. 




## SPY

Это внутренний формат spyql, он его переваривает как родной и очень быстро. 

```
sqlite3 -header -csv kladr.db "select * from DOMA;" | spyql "select * FROM csv TO spy" 2>/dev/null | wc -lc
3730618 749129912
```

**740МБайт** 

внутри это выглядит примерно так 

```
8004956700000000000000288c3a34d091d181d182d18032362c34d091d181d182d18032372c34d091d181d182d18032382c34d091d181d182d18032392c34d091d181d182d18033948c00948c06d094d09ed09c948a08375d01a68f4563014aeedf05004b6468018a05401cab7c1274942e
8004956600000000000000288c3934d091d181d182d18033302c34d091d181d182d18033312c34d091d181d182d18033322c34d091d181d182d180342c34d091d181d182d18035948c00948c06d094d09ed09c948a08385d01a68f4563014aeedf05004b6468018a05401cab7c1274942e
8004956800000000000000288c3b34d091d181d182d180362c34d091d181d182d180372c34d091d181d182d180382c34d091d181d182d180392c352c362c36d181d182d18038352c37948c00948c06d094d09ed09c948a08395d01a68f4563014aeedf05004b6468018a05401cab7c1274942e
```

# Сравнение

важно не просто экономное хранение, но и возможность делать компактные патчи построчно. 

записи только добавляются ? - тогда патчи для полей, которые в случае построчного добавления информации будут большими будут редко появляться (никогда). 

если смена значений в полях все же будет, то построчное хранение полей более разумно, но в этом случае тратится место на имена полей, как и в случае с json. 

если произойдет смена порядка полей или добавление поля - то патчи будут громоздкие, в этом случае логичны json форматы, но и они не спасают в полной мере. 

```
csv -> sql | sqlite-utils upsert // ignore 
```

это хороший вариант обновления даннных. 

не пустить строки которые НЕ нужны можно уже на этапе фильтрации csv `grep -v -f <file with patterns> ...`

# Итого 

**вариант 1**

* `jsonnl` + `gzip`	- хороший выбор, это компактно и удобно. 
* чтение: `find` поиск файлов для каждой таблицы, `zcat` их, ЛИБО `spyql` ЛИБО `sqlite-utils` импортируем в бд



вариант 2 

* `csv` - хороший выбор, компактное представление
* его преобразуем в sql через `spyql` или `sqlite-utils` 
* импорт
  * можно импортировать, пропуская
  * можно пропускать перед импортом, фильтруя сам дамп
  * можно получать patch между версиями из него делать csv и вствлять уже его



